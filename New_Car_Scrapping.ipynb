{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM7U1EMbzfYrpR9z/+3le7o"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Kf0rAhF9N5vw"},"outputs":[],"source":["# import sys\n","# !{sys.executable} -m pip install PyPDF2\n","\n","import requests\n","from bs4 import BeautifulSoup\n","import time\n","import io\n","# import PyPDF2\n","import json\n","import concurrent.futures\n","import pandas as pd"]},{"cell_type":"code","source":["def new_car_by_category():\n","\n","    url = \"https://www.pakwheels.com/new-cars/\"\n","\n","    headers = {\n","        'Accept': '*/*',\n","        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'\n","    }\n","\n","    response = requests.get(url, headers=headers)\n","    html_source = response.text\n","    soup = BeautifulSoup(html_source, 'html.parser')\n","\n","    alldetails = []\n","    extracted_links = []\n","    #Find all 'a' tags with the specified class for each make/company of cars\n","    ul = soup.find_all('ul',class_='make-list col-sm-2 list-unstyled new-car-list')\n","\n","    for each in ul:\n","      #Extract href attribute from each link\n","      extracted_links.append(url + each.find('a').get('href').split('/')[2])\n","\n","    for link in extracted_links:\n","\n","      response = requests.get(link, headers=headers)\n","      html_source = response.text\n","      soup = BeautifulSoup(html_source, 'html.parser')\n","\n","      #find all new car links of each make/company\n","      ul = soup.find_all('ul', class_='list-unstyled model-list row item clearfix')[0]\n","      li = ul.find_all('li')\n","      for each in li:\n","        #Extract href attribute from each link\n","        carurl = link + \"/\" + each.find('a').get('href').split('/')[3]\n","\n","        response = requests.get(carurl, headers=headers)\n","        html_source = response.text\n","        soup = BeautifulSoup(html_source, 'html.parser')\n","\n","        cardetails={}\n","        cardetails['title'] = carurl.split('/')[4] + \" \" + carurl.split('/')[5]\n","\n","        #extract all data from table\n","        try:\n","          info_table = soup.find('table', class_='bike-version-detailscont')\n","          info_rows = info_table.find_all('tr')\n","          # Iterate through table rows\n","          for row in info_rows:\n","              columns = row.find_all('td')\n","              if len(columns) == 2:\n","                  label = columns[0].text.strip()\n","                  value = columns[1].text.strip()\n","                  cardetails[label] = value\n","        except:\n","          print('Table not found for ' + cardetails['title'])\n","          break\n","\n","        print(cardetails)\n","\n","\n","        # litags = soup.find('ul',class_='gallery light-gallery list-unstyled cS-hidden').find_all('li')\n","        # images = []\n","        # for li in litags:\n","        #   images.append(li.get('data-src'))\n","        # cardetails['images'] = images\n","        alldetails.append(cardetails)\n","\n","     #Create a DataFrame from the car details\n","    df = pd.DataFrame(alldetails)\n","    # Save the DataFrame to an Excel file\n","    df.to_excel('new_car_details.xlsx', index=False)\n","\n","    print(\"Car details saved to 'new_car_details.xlsx'\")"],"metadata":{"id":"nv3bJ9gLOHXP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_car_by_category()"],"metadata":{"id":"KauqJf6MOiX9"},"execution_count":null,"outputs":[]}]}